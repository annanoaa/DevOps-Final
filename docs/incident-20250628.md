# Post-Mortem Report
**Incident ID:** INCIDENT-20250628-01
**Date:** 2025-06-28
**Severity:** HIGH

## Executive Summary
At approximately 23:39 (UTC+4), the backend API service began returning a high rate of HTTP 500 errors, causing a temporary service degradation for the frontend application. The incident was detected immediately via the application's incident logging system. The root cause was identified as a simulated critical error triggered from the incident simulation endpoint. The service recovered automatically, and the total impact duration was approximately 2 minutes.

### Incident Details
- **Start Time:** 2025-06-28 23:39:50
- **End Time:** 2025-06-28 23:41:50 (Assumed 2-minute recovery)
- **Duration:** 2 minutes
- **Affected Services:** `backend`, `frontend`
- **Impact:** The frontend application's system status was reported as "DEGRADED" and the backend returned 500 errors.

## Timeline

### Detection
- **Time:** 23:39:50
- **Method:** Incident logged automatically in the "Incident Log" on the frontend dashboard. Anomaly also visible in Grafana error rate dashboard.
- **Detected By:** Application's built-in monitoring and Grafana.

### Response
- **Time:** 23:40:00
- **Actions Taken:**
    - Confirmed the "DEGRADED" status on the frontend dashboard.
    - Reviewed the backend container logs (`docker-compose logs backend`).
- **Escalation:** N/A (Resolved by first responder)

### Resolution
- **Time:** 23:41:50
- **Resolution Method:** The system recovered automatically as the simulated error was transient.
- **Verification:** Observed that the "Backend Status" on the frontend UI returned to "healthy".

## Root Cause Analysis

### Primary Cause
The incident was triggered by a user clicking the **"Simulate Service Failure"** button on the Incident Simulation page. This action sends a request to the `/api/error` endpoint on the backend server, which is designed to intentionally throw a 500 error.

### Why It Happened
This was a planned incident simulation to test the monitoring, logging, and response procedures of the DevOps pipeline.

## Lessons Learned

### What Went Well
- **Monitoring was effective:** The application's own incident log detected and displayed the failure instantly.
- **Logs were accessible and clear:** The backend logs provided a precise error message.
- **Fast detection time:** The time from incident start to detection was immediate.

### What Could Be Improved
- **Automated Alerting:** While the dashboards showed the error, the system should be configured to send an automated notification (e.g., via email or Slack) to an on-call engineer so that no one needs to be actively watching a screen.

## Action Items

### Short-term Actions (Next week)
- [ ] Configure